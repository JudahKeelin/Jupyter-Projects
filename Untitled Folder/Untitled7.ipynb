{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26c5d2dc-35be-4d5c-a273-600b89d8637b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "Best MLP Params: {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (128, 64), 'learning_rate': 'constant'}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The feature names should match those that were passed during fit.\nFeature names seen at fit time, yet now missing:\n- emotion_encoded\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 102\u001b[39m\n\u001b[32m     98\u001b[39m x_scaled_h = scaler_h.transform(x_sample_h)\n\u001b[32m     99\u001b[39m x_scaled_a = scaler_a.transform(x_sample_a)\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m x_scaled_h = \u001b[43mscaler_h\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_sample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    103\u001b[39m x_scaled_a = scaler_a.transform(x_sample)\n\u001b[32m    105\u001b[39m happy_conf = happy_model.predict_proba(x_scaled_h)[\u001b[32m0\u001b[39m][\u001b[32m1\u001b[39m]  \u001b[38;5;66;03m# P(happy)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:319\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    317\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    321\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    322\u001b[39m         return_tuple = (\n\u001b[32m    323\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    324\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    325\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:1062\u001b[39m, in \u001b[36mStandardScaler.transform\u001b[39m\u001b[34m(self, X, copy)\u001b[39m\n\u001b[32m   1059\u001b[39m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m   1061\u001b[39m copy = copy \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.copy\n\u001b[32m-> \u001b[39m\u001b[32m1062\u001b[39m X = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1063\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1064\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1065\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1066\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1067\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1068\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1069\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1070\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mallow-nan\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1071\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1073\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sparse.issparse(X):\n\u001b[32m   1074\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.with_mean:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2919\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2835\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mvalidate_data\u001b[39m(\n\u001b[32m   2836\u001b[39m     _estimator,\n\u001b[32m   2837\u001b[39m     /,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2843\u001b[39m     **check_params,\n\u001b[32m   2844\u001b[39m ):\n\u001b[32m   2845\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Validate input data and set or check feature names and counts of the input.\u001b[39;00m\n\u001b[32m   2846\u001b[39m \n\u001b[32m   2847\u001b[39m \u001b[33;03m    This helper function should be used in an estimator that requires input\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2917\u001b[39m \u001b[33;03m        validated.\u001b[39;00m\n\u001b[32m   2918\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2919\u001b[39m     \u001b[43m_check_feature_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_estimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2920\u001b[39m     tags = get_tags(_estimator)\n\u001b[32m   2921\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m tags.target_tags.required:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2777\u001b[39m, in \u001b[36m_check_feature_names\u001b[39m\u001b[34m(estimator, X, reset)\u001b[39m\n\u001b[32m   2774\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m missing_names \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m unexpected_names:\n\u001b[32m   2775\u001b[39m     message += \u001b[33m\"\u001b[39m\u001b[33mFeature names must be in the same order as they were in fit.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m2777\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(message)\n",
      "\u001b[31mValueError\u001b[39m: The feature names should match those that were passed during fit.\nFeature names seen at fit time, yet now missing:\n- emotion_encoded\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# === Load and preprocess ===\n",
    "df = pd.read_csv(\"combined_features.csv\")\n",
    "\n",
    "# Encode emotion labels\n",
    "le = LabelEncoder()\n",
    "df[\"emotion_encoded\"] = le.fit_transform(df[\"emotion\"])\n",
    "emotion_labels = le.classes_\n",
    "\n",
    "# Focus on 'happy' and 'angry'\n",
    "happy_idx = np.where(emotion_labels == 'happy')[0][0]\n",
    "angry_idx = np.where(emotion_labels == 'angry')[0][0]\n",
    "\n",
    "df_filtered = df[df[\"emotion_encoded\"].isin([happy_idx, angry_idx])]\n",
    "X = df_filtered.drop(columns=[\"emotion\", \"emotion_encoded\"])\n",
    "y = df_filtered[\"emotion_encoded\"]\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# === Step 1: MLP Grid Search ===\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(128,), (256,), (128, 64)],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'alpha': [0.0001, 0.001],\n",
    "    'learning_rate': ['constant', 'adaptive']\n",
    "}\n",
    "\n",
    "mlp = MLPClassifier(max_iter=500, random_state=42)\n",
    "grid = GridSearchCV(mlp, param_grid, cv=3, n_jobs=-1, verbose=1)\n",
    "grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "best_mlp = grid.best_estimator_\n",
    "print(\"Best MLP Params:\", grid.best_params_)\n",
    "\n",
    "# === Step 2: Train directional classifiers ===\n",
    "\n",
    "# A) Happy-focused model (happy = 1, all else = 0)\n",
    "df_happy = df.copy()\n",
    "df_happy[\"happy_label\"] = df_happy[\"emotion\"].apply(lambda x: 1 if x == \"happy\" else 0)\n",
    "X_happy = df_happy.drop(columns=[\"emotion\", \"happy_label\"])\n",
    "y_happy = df_happy[\"happy_label\"]\n",
    "\n",
    "X_h_train, X_h_test, y_h_train, y_h_test = train_test_split(X_happy, y_happy, stratify=y_happy, test_size=0.2, random_state=42)\n",
    "scaler_h = StandardScaler()\n",
    "X_h_train_scaled = scaler_h.fit_transform(X_h_train)\n",
    "X_h_test_scaled = scaler_h.transform(X_h_test)\n",
    "\n",
    "happy_model = MLPClassifier(**grid.best_params_, max_iter=500, random_state=42)\n",
    "happy_model.fit(X_h_train_scaled, y_h_train)\n",
    "\n",
    "# B) Angry-focused model (angry = 1, all else = 0)\n",
    "df_angry = df.copy()\n",
    "df_angry[\"angry_label\"] = df_angry[\"emotion\"].apply(lambda x: 1 if x == \"angry\" else 0)\n",
    "X_angry = df_angry.drop(columns=[\"emotion\", \"angry_label\"])\n",
    "y_angry = df_angry[\"angry_label\"]\n",
    "\n",
    "X_a_train, X_a_test, y_a_train, y_a_test = train_test_split(X_angry, y_angry, stratify=y_angry, test_size=0.2, random_state=42)\n",
    "scaler_a = StandardScaler()\n",
    "X_a_train_scaled = scaler_a.fit_transform(X_a_train)\n",
    "X_a_test_scaled = scaler_a.transform(X_a_test)\n",
    "\n",
    "angry_model = MLPClassifier(**grid.best_params_, max_iter=500, random_state=42)\n",
    "angry_model.fit(X_a_train_scaled, y_a_train)\n",
    "\n",
    "# === Step 3: Hybrid Prediction ===\n",
    "\n",
    "hybrid_preds = []\n",
    "true_labels = []\n",
    "\n",
    "# Use same filtered happy/angry test set\n",
    "for i in range(len(X_test)):\n",
    "    x_sample = X_test.iloc[i:i+1]\n",
    "    true_label = y_test.iloc[i]\n",
    "\n",
    "    # Get the columns used to fit the happy scaler\n",
    "    expected_cols_h = X_h_train.columns\n",
    "    expected_cols_a = X_a_train.columns\n",
    "    \n",
    "    # Align x_sample to those columns\n",
    "    x_sample_h = x_sample.reindex(columns=expected_cols_h, fill_value=0)\n",
    "    x_sample_a = x_sample.reindex(columns=expected_cols_a, fill_value=0)\n",
    "    \n",
    "    # Now transform\n",
    "    x_scaled_h = scaler_h.transform(x_sample_h)\n",
    "    x_scaled_a = scaler_a.transform(x_sample_a)\n",
    "\n",
    "\n",
    "    x_scaled_h = scaler_h.transform(x_sample)\n",
    "    x_scaled_a = scaler_a.transform(x_sample)\n",
    "\n",
    "    happy_conf = happy_model.predict_proba(x_scaled_h)[0][1]  # P(happy)\n",
    "    angry_conf = angry_model.predict_proba(x_scaled_a)[0][1]  # P(angry)\n",
    "\n",
    "    pred = happy_idx if happy_conf > angry_conf else angry_idx\n",
    "    hybrid_preds.append(pred)\n",
    "    true_labels.append(true_label)\n",
    "\n",
    "# === Step 4: Evaluation ===\n",
    "print(\"\\n=== Hybrid Model Evaluation ===\")\n",
    "print(classification_report(true_labels, hybrid_preds, target_names=[\"angry\", \"happy\"]))\n",
    "\n",
    "cm = confusion_matrix(true_labels, hybrid_preds)\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[\"angry\", \"happy\"], yticklabels=[\"angry\", \"happy\"])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Hybrid Classifier Confusion Matrix\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
